---
title: "Estadística Descriptiva"
subtitle: "Visualización de Datos y Teoremas Importantes"
author: "Christian F. Badillo Hernández"
date: 01/08/24
lang: "es"
date-format: "D MMM YYYY"
format:
    revealjs:
        incremental: true
        scrollable: true
        smaller: false   
        theme: night
        logo: img/Lab25_logo_2015.png
        footer: "Temas Selectos en Estadística"
        preview-links: true
        preload-iframes: true
        slide-number: true
        transition: convex
        background-transition: fade
        transition-speed: slow
        navigation-mode: linear
        touch: true
        controls: true
        embed-resources: true
        page-layout: custom
        reference-location: document
        link-external-newwindow: true
engine: knirt
server: shiny
runtime: shiny
---

# Momentos

> "The death of a men is a tragedy, the death of millions is a statistic." - Joseph Stalin

## Definición {.smaller}

* Todas las distribuciones de probabilidad tienen un conjunto de características (parámetros) que las describen. Estas características se conocen como **momentos**.

* Normalmente se suelen presentar como **medidas de tendencia central** y **medidas de dispersión**. Pero el concepto de momento es más general.

* Cada momento estadístico se puede calcular utilizando la **función generadora de momentos**, que se define para cada distribución de probabilidad. 

* Debido a que su uso requiere mayor conocimiento de cálculo, no se abordará en este curso. Pero se presentaran los momentos más comunes de la forma habitual.

* Quien desee profundizar en el tema puede consultar la sección $4.4$ del libro ***["Probability and Statistics"](http://www.economia.unam.mx/biblioteca/Pdf/Morris%20H%20DeGroot_%20Mark%20J%20Schervish-Probability%20and%20statistics-Pearson%20Education%20%20(2012).pdf)*** de **Morris H. DeGroot**.

## Valor Esperado {.smaller}

* El ***valor esperado*** (esperanza matemática, media) de una v.a discreta $X$ se define como:
$$
E(X) = \mu = \sum_{i=1}^n x_i p_i
$$

* El ***valor esperado*** (esperanza matemática, media) de una v.a continua $X$ se define como:
$$
E(X) = \mu = \int_{-\infty}^{\infty} x f(x) dx
$$

* El valor esperado es una medida de tendencia central. Y nos dice el valor promedio que se espera obtener de una v.a. y es la mejor estimación de la v.a. en el largo plazo.

* En ambos casos ponderamos cada valor de la v.a. por su probabilidad de ocurrencia.

* El valor esperado también se puede interpretar como el centro de gravedad de la distribución de probabilidad y define al *primer momento* de la distribución.

## Valor Esperado {.smaller}

* Existe otra forma de calcular el valor esperado de una v.a. discreta $X$:
$$
E(X) = \mu = \frac{\sum_{i=1}^n x_i}{n}
$$

* Es decir, el valor esperado es igual a la suma de todos los valores de la v.a. dividido entre el número de valores. Ambas formas son equivalentes.

* Este segundo método es más sencillo de calcular y sirve para estimar el valor esperado de una v.a. cuando no se conoce la función de probabilidad.

## Ejemplo 1: Valor Esperado {.smaller}

* Supongamos que tenemos una v.a. discreta $X$ con la siguiente función de probabilidad:
$$
\begin{array}{c|c|c|c|c}
x_i & 1 & 2 & 3 & 4 \\ \hline
p_i & 0.1 & 0.2 & 0.3 & 0.4
\end{array}
$$

* Entonces el valor esperado de $X$ es:
$$
\begin{align*}
E(X) &= \mu = \sum_{i=1}^n x_i p_i = \\
&1(0.1) + 2(0.2) + 3(0.3) + 4(0.4) \\
&= 3
\end{align*}
$$

* Es decir, en promedio se espera obtener $3$ unidades de $X$.

## Ejemplo 2: Valor Esperado {.smaller}

* Si tenemos un dado de $6$ caras, entonces la v.a. $X$ que representa el resultado de lanzar el dado tiene la siguiente función de probabilidad:
$$
\begin{array}{c|c|c|c|c|c|c}
x_i & 1 & 2 & 3 & 4 & 5 & 6 \\ \hline
p_i & \frac{1}{6} & \frac{1}{6} & \frac{1}{6} & \frac{1}{6} & \frac{1}{6} & \frac{1}{6}
\end{array}
$$

* ¿Cuál es el valor esperado de $X$?

* Respuesta:
$$
\begin{align*}
E(X) &= \mu = \sum_{i=1}^n x_i p_i \\
&= 1\left(\frac{1}{6}\right) + 2\left(\frac{1}{6}\right) + 3\left(\frac{1}{6}\right) \\
&+ 4\left(\frac{1}{6}\right) + 5\left(\frac{1}{6}\right) + 6\left(\frac{1}{6}\right) \\
&= \frac{21}{6} = 3.5
\end{align*}
$$

## Ejemplo 3: Valor Esperado {.smaller}

* Si tenemos una v.a. bernoulli $X$ con $p = 0.3$, entonces su función de probabilidad es:
$$
\begin{array}{c|c|c}
x_i & 0 & 1 \\ \hline
p_i & 0.7 & 0.3
\end{array}
$$

* ¿Cuál es el valor esperado de $X$?

* Respuesta:
$$
\begin{align*}
E(X) &= \mu = \sum_{i=1}^n x_i p_i \\
&= 0(0.7) + 1(0.3) \\
&= 0.3
\end{align*}
$$

* En general, si tenemos una v.a. bernoulli $X$ con $p$, entonces su valor esperado es:
$$
E(X) = \mu = p
$$

## Ejemplo 4: Valor Esperado {.smaller}

* Si tenemos una v.a. uniforme continua $X$ con $a = 0$ y $b = 10$, entonces su función de densidad es:
$$
f(x) = \frac{1}{10 - 0} = \frac{1}{10}
$$

* Su valor esperado se calcula como:
$$
\begin{align*}
E(X) &= \mu = \int_{-\infty}^{\infty} x f(x) dx \\
&= \int_{0}^{10} x \frac{1}{10} dx \\
&= \frac{1}{10} \int_{0}^{10} x dx \\
&= \frac{1}{10} \left[\frac{x^2}{2}\right]_0^{10} \\
&= \frac{1}{10} \left[\frac{10^2}{2} - \frac{0^2}{2}\right] \\
&= \frac{100}{20} = 5
\end{align*}
$$

* En general, si tenemos una v.a. uniforme continua $X$ con $a$ y $b$, entonces su valor esperado es:
$$
E(X) = \mu = \frac{a + b}{2}
$$

## Ejemplo 5: Valor Esperado {.smaller}

* Si tenemos una v.a. exponencial $X$ con $\lambda = 0.5$, entonces su función de densidad es:
$$
f(x) = \lambda e^{-\lambda x} = 0.5 e^{-0.5 x}
$$

* Su valor esperado es:
$$
\begin{align*}
E(X) &= \mu = \int_{-\infty}^{\infty} x f(x) dx \\
&= \int_{0}^{\infty} x 0.5 e^{-0.5 x} dx \\
&= 0.5 \int_{0}^{\infty} x e^{-0.5 x} dx \\
&= 0.5 \left[-2 e^{-0.5 x} (x + 2)\right]_0^{\infty} \\
&= 0.5 \left[-2 e^{-0.5 \infty} (\infty + 2) - (-2 e^{-0.5 0} (0 + 2))\right] \\
&= 0.5 \left[-2 (0) (\infty + 2) - (-2 (1) (0 + 2))\right] \\
&= 0.5 \left[0 - (-2 (1) (2))\right] \\
&= 0.5 \left[4\right] \\
&= 2
\end{align*}
$$

En general, si tenemos una v.a. exponencial $X$ con $\lambda$, entonces su valor esperado es:
$$
E(X) = \mu = \frac{1}{\lambda}
$$

##

| Distribución | Valor esperado |
|:------------:|:--------------:|
| Bernoulli | $p$ |
| Binomial | $np$ |
| Geométrica | $\frac{1}{p}$ |
| Poisson | $\lambda$ |
| Uniforme discreta | $\frac{a + b}{2}$ |
| Uniforme continua | $\frac{a + b}{2}$ |
| Exponencial | $\frac{1}{\lambda}$ |
| Normal | $\mu$ |
| Normal estándar | $0$ |
| Log-normal | $e^{\mu + \frac{\sigma^2}{2}}$ |
| Chi-cuadrada | $k$ |
| Beta | $\frac{\alpha}{\alpha + \beta}$ |
| T de Student | $0$ |
| F de Fisher-Snedecor | $\frac{\nu_2}{\nu_2 - 2}$ |
| Gamma | $\frac{\alpha}{\lambda}$ |

## Varianza {.smaller}

* La ***variabilidad*** de una v.a. $X$ se mide con la ***varianza***, que se define como:
$$
Var(X) = \sigma^2 = E\left[(X - \mu)^2\right]
$$

* Representa el promedio de la distancia al cuadrado de cada valor de la v.a. con respecto a su valor esperado. En otras palabras, mide la dispersión de los valores de la v.a. con respecto a la media.

* La varianza pertenece a los ***momentos centrales*** de la distribución. Estos se definen como:
$$
\mu_k = E\left[(X - \mu)^k\right]
$$

* El primer momento central es la varianza.

## Desviación Estándar {.smaller}

* La ***desviación estándar*** de una v.a. $X$ se define como:
$$
\sigma = \sqrt{Var(X)} = \sqrt{\sigma^2} = \sqrt{E\left[(X - \mu)^2\right]}
$$

* Es la raíz cuadrada de la varianza y representa la distancia promedio de cada valor de la v.a. con respecto a su valor esperado en su misma unidad de medida.

* La desviación estándar es más fácil de interpretar que la varianza, pero es menos útil para realizar cálculos, muchos modelos estadísticos requieren la varianza.

## Ejemplo 1: Varianza {.smaller}

* Supongamos que tenemos una v.a. discreta $X$ con la siguiente función de probabilidad:
$$
\begin{array}{c|c|c|c|c}
x_i & 1 & 2 & 3 & 4 \\ \hline
p_i & 0.1 & 0.2 & 0.3 & 0.4
\end{array}
$$

* Anteriormente calculamos que el valor esperado de $X$ es $3$. Entonces la varianza de $X$ es:
$$
\begin{align*}
Var(X) &= \sigma^2 = E\left[(X - \mu)^2\right] \\
&= E\left[(X - 3)^2\right] \\
&= \sum_{i=1}^n (x_i - 3)^2 p_i \\
&= (1 - 3)^2(0.1) + (2 - 3)^2(0.2) \\
&+ (3 - 3)^2(0.3) + (4 - 3)^2(0.4) \\
&= 0.1 + 0.2 + 0.4 \\
&= 0.7
\end{align*}
$$

* Y la varianza estándar de $X$ es:
$$
\sigma = \sqrt{Var(X)} = \sqrt{0.7} = 0.8367
$$

## Varianza muestral {.smaller}

* Cuando no se conoce la función de probabilidad de una v.a. discreta $X$, se puede estimar la varianza de la siguiente forma:
$$
\begin{align*}
Var(X) &= \sigma^2 = E\left[(X - \mu)^2\right] \\
&= \frac{\sum_{i=1}^n (x_i - \mu)^2}{n-1}
\end{align*}
$$

* Es decir, se calcula el valor esperado de la v.a. utilizando el segundo método y se divide entre $n-1$, donde $n$ es el número de observaciones. Esta varianza se conoce como ***varianza muestral***.

* El factor $n-1$ se conoce como ***grados de libertad*** y se utiliza para corregir el sesgo de la estimación.

* La varianza muestral es un estimador insesgado de la varianza poblacional. Es decir, en promedio la varianza muestral es igual a la varianza poblacional.

##

```{r}
#| panel: input
sliderInput("n", "Número de observaciones:", min = 2, max = 150, value = 10, step = 1)
```

```{r}
```

```{r}
#| context: server
library(ggplot2)
library(ggdark)
output$var <- renderPlot({
    n <- input$n
    set.seed(1234)
    samples <- rnorm(n)
    x <- seq(-6, 6, 0.01)

    sample_var_bias <- sum((samples - mean(samples))^2) / n
    sample_var_unbias <- sum((samples - mean(samples))^2) / (n - 1)
    
    df <- data.frame(x = x, y = dnorm(x))
    df2 <- data.frame(x = samples, y = rep(0, n))

    ggplot(df, aes(x = x, y = y)) +
        geom_line(color = "white") +
        geom_point(data = df2, aes(x = x, y = y), color = "white") +

        geom_vline(xintercept = mean(samples), color = "pink") +
        geom_vline(xintercept = mean(samples) + sqrt(sample_var_bias), color = "red") +
        geom_vline(xintercept = mean(samples) - sqrt(sample_var_bias), color = "red") +
        geom_vline(xintercept = mean(samples) + sqrt(sample_var_unbias), color = "green") +
        geom_vline(xintercept = mean(samples) - sqrt(sample_var_unbias), color = "green") +

        geom_text(aes(x = mean(samples) + sqrt(sample_var_bias), 
        y = 0.1, label = "Desviación estándar Sesgada"), 
        color = "red", size = 10) +
        geom_text(aes(x = mean(samples) - sqrt(sample_var_unbias), 
        y = 0.25, label = "Desviación estándar Insesgada"), 
        color = "green", size = 10) +
        geom_text(aes(x = mean(samples), 
        y = 0.35, label = "Media Muestral"), 
        color = "pink", size = 10) +

        scale_x_continuous(breaks = seq(-6, 6, 1)) +
        dark_theme_gray() +
        labs(x = "x", y = "f(x)") +
        theme(axis.text = element_text(size = 30),
            axis.title = element_text(size = 40),
            plot.title = element_text(size = 50))
})

```

```{r}
#| panel: fill  
plotOutput("var", width = "100%", height = "590px")
```

## Ejemplo 2: Varianza muestral {.smaller}

* Imaginemos que realiza una encuesta a $10$ personas y les pregunta su edad. Los resultados son los siguientes:
$$
\begin{array}{c|c|c|c|c|c|c|c|c|c|c}
x_i & 18 & 19 & 20 & 21 & 22 & 23 & 24 & 25 & 26 & 27 \\ \hline
\end{array}
$$

* ¿Cuál es la varianza muestral de la edad de las personas encuestadas?

* Respuesta:
$$
\begin{align*}
Var(X) &= \sigma^2 = E\left[(X - \mu)^2\right] \\
&= \frac{\sum_{i=1}^n (x_i - \mu)^2}{n-1} \\
&= \frac{(18 - 22)^2 + (19 - 22)^2 + (20 - 22)^2}{9} \\
&+ \frac{(21 - 22)^2 + (22 - 22)^2}{9} \\
&+ \frac{(23 - 22)^2 + (24 - 22)^2 + (25 - 22)^2}{9} \\
&+ \frac{(26 - 22)^2 + (27 - 22)^2}{9} \\
&= \frac{16 + 9 + 4 + 1 + 0 + 1 + 4 + 9 + 16 + 25}{9} \\
&= \frac{85}{9} = 9.4444
\end{align*}
$$

## Ejemplo 3: Varianza muestral {.smaller}

* Si tenemos una v.a. bernoulli $X$ con $p = 0.3$.
$$
\begin{array}{c|c|c}
x_i & 0 & 1 \\ \hline
p_i & 0.7 & 0.3
\end{array}
$$

* ¿Cuál es la varianza de $X$?

* Respuesta:
$$
\begin{align*}
Var(X) &= \sigma^2 = E\left[(X - \mu)^2\right] \\
&= E\left[(X - 0.3)^2\right] \\
&= (0 - 0.3)^2(0.7) + (1 - 0.3)^2(0.3) \\
&= 0.063 + 0.147 \\
&= 0.21
\end{align*}
$$

* Su desviación estándar es: $\sigma = \sqrt{0.21} = 0.4583$

* En general, si tenemos una v.a. bernoulli $X$ con $p$, entonces su varianza es:
$$
Var(X) = \sigma^2 = p(1 - p)
$$

## Ejemplo 4: Varianza muestral {.smaller}

* Si tenemos una v.a. uniforme continua $X$ con $a = 0$ y $b = 10$, entonces:
$$
f(x) = \frac{1}{10 - 0} = \frac{1}{10}
$$

* Su varianza se calcula como:
$$
\begin{align*}
Var(X) &= \sigma^2 = E\left[(X - \mu)^2\right] \\
&= \int_{-\infty}^{\infty} (x - \mu)^2 f(x) dx \\
&= \int_{0}^{10} (x - 5)^2 \frac{1}{10} dx \\
&= \frac{1}{10} \int_{0}^{10} (x - 5)^2 dx \\
&= \frac{1}{10} \left[\frac{(x - 5)^3}{3}\right]_0^{10} \\
&= \frac{1}{10} \left[\frac{(10 - 5)^3}{3} - \frac{(0 - 5)^3}{3}\right] \\
&= \frac{1}{10} \left[\frac{125}{3} - \frac{-125}{3}\right] \\
&= \frac{1}{10} \left[\frac{250}{3}\right] \\
&= \frac{25}{3} = 8.3333
\end{align*}
$$

* Y su desviación estándar es: $\sigma = \sqrt{8.3333} = 2.8868$

* En general, si tenemos una v.a. uniforme continua $X$ con $a$ y $b$, entonces su varianza es:
$$
Var(X) = \sigma^2 = \frac{(b - a)^2}{12}
$$

##

| Distribución | Varianza |
|:------------:|:--------------:|
| Bernoulli | $p(1 - p)$ |
| Binomial | $np(1 - p)$ |
| Geométrica | $\frac{1 - p}{p^2}$ |
| Poisson | $\lambda$ |
| Uniforme discreta | $\frac{n^2 - 1}{12}$ |
| Uniforme continua | $\frac{(b - a)^2}{12}$ |
| Exponencial | $\frac{1}{\lambda^2}$ |
| Normal | $\sigma^2$ |
| Normal estándar | $1$ |
| Log-normal | $e^{2\mu + \sigma^2}(e^{\sigma^2} - 1)$ |
| Chi-cuadrada | $2k$ |
| Beta | $\frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}$ |
| T de Student | $\frac{\nu_2}{\nu_2 - 2}$ |
| F de Fisher-Snedecor | $\frac{2\nu_2^2(\nu_1 + \nu_2 - 2)}{\nu_1(\nu_2 - 2)^2(\nu_2 - 4)}$ |
| Gamma | $\frac{\alpha}{\lambda^2}$ |
